{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd279114",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbc0980d",
   "metadata": {},
   "source": [
    "<h1> Strategic Analysis </h1>\n",
    "<h4> Strategic Business Information Technology & Problem Solving for Industry (Capstone Project) </h4>\n",
    "<p> Lecturers: Ken Healy & Muhammad Iqbal </p>\n",
    "<p> Students: Luiza Cavalcanti Albuquerque Brayner (2020309) & Edgard Pacheco (2020332)  </p>\n",
    "\n",
    "<b>Solutions, Findings, and Demonstrations. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f549c5",
   "metadata": {},
   "source": [
    "<h2> Introduction </h2>\n",
    "<b> In this part of the project, we will be providing a comprehensive solution for a personalized health track and disease risk prediction application. We will demonstrate our data collection, cleanning and visualization  of the fidings, AI models to be used, providing a clear overview of our innovative approach. In this project, we will focus over the prediction of a specific risk of developing cardiovascular diseases, by making use of predictive models, we will make a comparison of some model's performance in making such prediction, and justification of the selection of the final model. </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f538356",
   "metadata": {},
   "source": [
    "<h3> Data Gathering Process </h3>\n",
    "<p> Ref.[1]</p>\n",
    "<p> User data, will be gathered by a dataset that is previosly done, with some information that would be gathered from the user's input. All user data currently being analysed is fictional, and in the future could be altered to appropriately match the application goals, which offers options for manual input, and therefore provide a personalized response to better user experience. There will also be an upfront form that will be required for user input of personal and health data, that is where the data will come from to do the first prediction. But for testing purposes, we will be training the models over a fictional dataset, mainly created by a snipped of code below. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a816dbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Smoking Years</th>\n",
       "      <th>Alcohol Consumption</th>\n",
       "      <th>Physical Activity</th>\n",
       "      <th>Diet</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Diagnosed Disease</th>\n",
       "      <th>Total Cholesterol (mg/dL)</th>\n",
       "      <th>LDL Cholesterol (mg/dL)</th>\n",
       "      <th>HDL Cholesterol (mg/dL)</th>\n",
       "      <th>Triglycerides (mg/dL)</th>\n",
       "      <th>Systolic Blood Pressure (mmHg)</th>\n",
       "      <th>Diastolic Blood Pressure (mmHg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>Female</td>\n",
       "      <td>10</td>\n",
       "      <td>Never</td>\n",
       "      <td>Intense</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>Yes</td>\n",
       "      <td>None</td>\n",
       "      <td>178</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>210</td>\n",
       "      <td>171</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>Female</td>\n",
       "      <td>8</td>\n",
       "      <td>Never</td>\n",
       "      <td>Intense</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>227</td>\n",
       "      <td>105</td>\n",
       "      <td>22</td>\n",
       "      <td>245</td>\n",
       "      <td>143</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>Other</td>\n",
       "      <td>5</td>\n",
       "      <td>Never</td>\n",
       "      <td>None</td>\n",
       "      <td>Balanced</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>170</td>\n",
       "      <td>64</td>\n",
       "      <td>43</td>\n",
       "      <td>258</td>\n",
       "      <td>175</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>Never</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Vegan</td>\n",
       "      <td>Yes</td>\n",
       "      <td>None</td>\n",
       "      <td>206</td>\n",
       "      <td>79</td>\n",
       "      <td>46</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Other</td>\n",
       "      <td>18</td>\n",
       "      <td>Never</td>\n",
       "      <td>Light</td>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>216</td>\n",
       "      <td>131</td>\n",
       "      <td>78</td>\n",
       "      <td>208</td>\n",
       "      <td>178</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender  Smoking Years Alcohol Consumption Physical Activity  \\\n",
       "0   31  Female             10               Never           Intense   \n",
       "1   36  Female              8               Never           Intense   \n",
       "2   24   Other              5               Never              None   \n",
       "3   41    Male              1               Never          Moderate   \n",
       "4   28   Other             18               Never             Light   \n",
       "\n",
       "         Diet Obesity Diagnosed Disease  Total Cholesterol (mg/dL)  \\\n",
       "0   Unhealthy     Yes              None                        178   \n",
       "1   Unhealthy      No              None                        227   \n",
       "2    Balanced      No              None                        170   \n",
       "3       Vegan     Yes              None                        206   \n",
       "4  Vegetarian      No              None                        216   \n",
       "\n",
       "   LDL Cholesterol (mg/dL)  HDL Cholesterol (mg/dL)  Triglycerides (mg/dL)  \\\n",
       "0                       62                       62                    210   \n",
       "1                      105                       22                    245   \n",
       "2                       64                       43                    258   \n",
       "3                       79                       46                    117   \n",
       "4                      131                       78                    208   \n",
       "\n",
       "   Systolic Blood Pressure (mmHg)  Diastolic Blood Pressure (mmHg)  \n",
       "0                             171                               93  \n",
       "1                             143                               89  \n",
       "2                             175                              115  \n",
       "3                             117                               90  \n",
       "4                             178                               71  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading the data from a previous dataset \n",
    "df = pd.read_csv('Health_Data_50k_English.csv')\n",
    "\n",
    "# Data preparation\n",
    "# Removing columns not used in the model - making the dataset personalized \n",
    "df.drop(['Full Name'], axis=1, inplace=True)\n",
    "\n",
    "# Printing first few rows of the dataset to be analysed \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d63765",
   "metadata": {},
   "source": [
    "<h3> Explanation of the code provided above to gather user data </h3>\n",
    "<p> Ref.[2] </p>\n",
    "<p> <b> 1. Collecting user data:</b> The main idea is to output a form where the user can input it's own health data and personal data, but for testing purposes as said above, we will be creating such data.</p>\n",
    "<p> <b> 2. Storing the data:</b> The data is gathered from a previous dataset and then adapted to our own usage, excluding columns that will not be in usage for the analyses. The data is then stored temporarely in a variable. </p>\n",
    "<p> <b> 3. Writting to CSV:</b> In the original idea, the code will also write the collected user data into a CSV file called \"user_data.csv\". The file is already created, and it writes on the following available roll. But following the test purpose, this step of the data gathering will be delayed for the implementation part. </p>\n",
    "<p> <b> 4. Confirmation:</b> Again, focusing over the original idea, after the data is added to the CSV file, then there will be a confirmation message, which will indicate that the data has been submited. But for testing purposes, we double check the dataframe, by printting the first few rows and analysing the output. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c891457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataset: (50000, 14)\n",
      "Missing values:\n",
      " Age                                0\n",
      "Gender                             0\n",
      "Smoking Years                      0\n",
      "Alcohol Consumption                0\n",
      "Physical Activity                  0\n",
      "Diet                               0\n",
      "Obesity                            0\n",
      "Diagnosed Disease                  0\n",
      "Total Cholesterol (mg/dL)          0\n",
      "LDL Cholesterol (mg/dL)            0\n",
      "HDL Cholesterol (mg/dL)            0\n",
      "Triglycerides (mg/dL)              0\n",
      "Systolic Blood Pressure (mmHg)     0\n",
      "Diastolic Blood Pressure (mmHg)    0\n",
      "dtype: int64\n",
      "Data types:\n",
      " Age                                 int64\n",
      "Gender                             object\n",
      "Smoking Years                       int64\n",
      "Alcohol Consumption                object\n",
      "Physical Activity                  object\n",
      "Diet                               object\n",
      "Obesity                            object\n",
      "Diagnosed Disease                  object\n",
      "Total Cholesterol (mg/dL)           int64\n",
      "LDL Cholesterol (mg/dL)             int64\n",
      "HDL Cholesterol (mg/dL)             int64\n",
      "Triglycerides (mg/dL)               int64\n",
      "Systolic Blood Pressure (mmHg)      int64\n",
      "Diastolic Blood Pressure (mmHg)     int64\n",
      "dtype: object\n",
      "Statistics (summary):\n",
      "                 Age  Smoking Years  Total Cholesterol (mg/dL)  \\\n",
      "count  50000.000000   50000.000000               50000.000000   \n",
      "mean      48.657020      19.522300                 199.332760   \n",
      "std       17.895249      11.510127                  28.879299   \n",
      "min       18.000000       0.000000                 150.000000   \n",
      "25%       33.000000      10.000000                 174.000000   \n",
      "50%       49.000000      20.000000                 199.000000   \n",
      "75%       64.000000      30.000000                 224.000000   \n",
      "max       79.000000      39.000000                 249.000000   \n",
      "\n",
      "       LDL Cholesterol (mg/dL)  HDL Cholesterol (mg/dL)  \\\n",
      "count             50000.000000              50000.00000   \n",
      "mean                 99.379560                 49.48520   \n",
      "std                  28.913989                 17.30154   \n",
      "min                  50.000000                 20.00000   \n",
      "25%                  74.000000                 34.00000   \n",
      "50%                  99.000000                 49.00000   \n",
      "75%                 125.000000                 64.00000   \n",
      "max                 149.000000                 79.00000   \n",
      "\n",
      "       Triglycerides (mg/dL)  Systolic Blood Pressure (mmHg)  \\\n",
      "count           50000.000000                    50000.000000   \n",
      "mean              199.365660                      144.528000   \n",
      "std                58.036432                       20.263532   \n",
      "min               100.000000                      110.000000   \n",
      "25%               149.000000                      127.000000   \n",
      "50%               199.000000                      144.000000   \n",
      "75%               250.000000                      162.000000   \n",
      "max               299.000000                      179.000000   \n",
      "\n",
      "       Diastolic Blood Pressure (mmHg)  \n",
      "count                     50000.000000  \n",
      "mean                         94.417000  \n",
      "std                          14.422579  \n",
      "min                          70.000000  \n",
      "25%                          82.000000  \n",
      "50%                          94.000000  \n",
      "75%                         107.000000  \n",
      "max                         119.000000  \n",
      "Number of duplicated rows: 0\n",
      "Number of outliers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/19n82dfx2jdcfq3h1tg_4jzh0000gn/T/ipykernel_5174/3305747109.py:34: FutureWarning: Automatic reindexing on DataFrame vs Series comparisons is deprecated and will raise ValueError in a future version.  Do `left, right = left.align(right, axis=1, copy=False)` before e.g. `left == right`\n",
      "  outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Gender: ['Female' 'Other' 'Male']\n",
      "Unique values in Alcohol Consumption: ['Never' 'Occasionally' 'Regularly']\n",
      "Unique values in Physical Activity: ['Intense' 'None' 'Moderate' 'Light']\n",
      "Unique values in Diet: ['Unhealthy' 'Balanced' 'Vegan' 'Vegetarian']\n",
      "Unique values in Obesity: ['Yes' 'No']\n",
      "Unique values in Diagnosed Disease: ['None' 'Lung Cancer']\n"
     ]
    }
   ],
   "source": [
    "# Import for checking plot of outliers \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dimensions\n",
    "print(\"Dimensions of the dataset:\", df.shape)\n",
    "\n",
    "# Missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Data types of each feature\n",
    "print(\"Data types:\\n\", df.dtypes)\n",
    "\n",
    "# Numerical features\n",
    "print(\"Statistics (summary):\\n\", df.describe())\n",
    "\n",
    "# Creating a new DataFrame (converting an existing object df into a DataFrame) \n",
    "# It is already a dataframe so basically it is a copy of it\n",
    "data = pd.DataFrame(df)\n",
    "\n",
    "# Get duplicated rows\n",
    "duplicated_rows = df[df.duplicated(keep=False)]\n",
    "# print(\"Duplicated rows:\")\n",
    "# print(duplicated_rows)\n",
    "\n",
    "# Count duplicated rows and output\n",
    "duplicated_counts = df.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicated rows:\", duplicated_counts)\n",
    "\n",
    "# Identify outliers\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "outliers_count = outliers.sum()\n",
    "print(\"Number of outliers:\", outliers_count)\n",
    "\n",
    "# Create a bar plot to show outliers visualization \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar([\"Outliers\"], [outliers_count], color='orange')\n",
    "plt.xlabel(\"Data Points\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of Outliers\")\n",
    "plt.show()\n",
    "\n",
    "# Check for unique values in categorical columns - data validation \n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Unique values in {column}:\", unique_values)\n",
    "\n",
    "# distribution of categorical features\n",
    "# print(\"Distribution of the feature 'Action':\\n\", df['Action'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d2782",
   "metadata": {},
   "source": [
    "<h3> Explanation of the code provided above to perform a data cleanning EDA (Exploratory Data Analysis) </h3> \n",
    "<p> Ref.[3] </p>\n",
    "<p> <b> 1. Identifying dataset dimentions: </b> The dataset to be used is formulated by 50000 instances and 14 features/attributes. Regarding the dimensions of the final dataset (specifically rows and columns), it was possible to ensure that all data was loaded corretly </p> \n",
    "<p> <b> 2. Identifying missing values: </b> By making use of '.isnull()' method, it was possible to identify missing values and then count them with the '.sum()' method to check missing values in each column. It is also possible to check that there was no records of missing values, but important to highlight that the values were also filled by 0, null and 'none' variables. Therefore, needing a deeper analysis over that. </p> \n",
    "<p> <b> 3. Checking data types:</b> There was a check done for all data types, it was used 'df.dtypes' to recognize all different data types to be used on the analysis and model training. </p> \n",
    "<p> <b> 4. Identifying and handling duplicated values:</b> It was done a deeper analysis over the duplicated values, the duplicated values returned an empty dataframe on the first try, which meant that there was no duplicated rows. Secondly it was requested a count of such repeted values, which then returned 0, confirming that there was no repeted values. </p> \n",
    "<p> <b> 5. Checking for data normalization and inconsistent values:</b> This step was also done while checking the data types, which included an overview of full dataframe data types for further analysis. See output of code above for further details. </p> \n",
    "<p> <b> 6. Identifying outliers:</b> It was creted over the last few lines of code an outliers detection and count, printing 0 on the output, as well as no outliers appered over the visualization, it was also created a graph to visualize such data, but no outliers were shown. </p> \n",
    "<b> Data Validation </b>\n",
    "<p> This step of the data cleaning was done a check for unique values by the end of †he code so far it was done just to give a general idea of the data to be used. This step outputed unique values found in Gender, Alcohool Consumption, Physical Activity, Diet, Obesity, and Diagnosed Disease. Such values were broken down and outputed together with each category.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0ad4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING FIX CODE ABOVE\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the features and labels\n",
    "X = df.drop('Diagnosed Disease', axis=1)\n",
    "y = df['Diagnosed Disease']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create preprocessing pipeline for numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create preprocessing pipeline for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps for both numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa6514",
   "metadata": {},
   "source": [
    "<h3> Explanation of the code provided above to perform the models trainings </h3> \n",
    "<p> Ref.[4] </p>\n",
    "<p> The code below preproces the dataset before training the machine learning model. </p> \n",
    "<p> <b> Defining features and labels: </b>Separating the features (X) from the target variable or labels (Y). </p>\n",
    "<p> <b> Splitting the data: </b>The dataset is then divided into training and testing sets using a function (train_test_split), which randomly splits the dataset into subsets for training and testing the model. The parameter 'test_size' specifies the propotion of the dataset to include in the test split, and 'random_state' ensures reproducubility. </p> \n",
    "<b> Definition of numerical and categorical features </b>\n",
    "<p> <b> Preprocessing pipelines: </b>For numerical features, the pipeline applies standardization using 'StandardScaler' to scale the features to have mean of 0 and a standard deviation of 1. For categorical features, the pipelone applies on-hot encoding using 'OneHotEncoder' to convert categotical variables into binary vectors. </p>\n",
    "<p> <b> Combination of preprocessing:</b>Combination of such steps were done through the 'ColumnTransformer', which applies specified transformations to the respective feature types. </p>\n",
    "<p> <b> Preprocessing data:</b>Training and testing sets are preprocessed using the 'ColumnTransformer' preprocessor. The method 'fit_transform' is used on the training set, and the 'transform' method used on the testing set, to ensure consistency in the preprocessing. </p>\n",
    "<p> Generally, the code above, prepares the dataset by scaling numerical features and encoding categorical features, making the data suitable for training the machine learning model. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d863eefc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (40000, 1), indices imply (40000, 33203)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bl/19n82dfx2jdcfq3h1tg_4jzh0000gn/T/ipykernel_5174/4033954526.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Convert preprocessed data to DataFrame for visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Ensure that the indices match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    709\u001b[0m                     )\n\u001b[1;32m    710\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    712\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (40000, 1), indices imply (40000, 33203)"
     ]
    }
   ],
   "source": [
    "# CHECK THIS PART... CODE VISUALIZATION OF THE ABOVE SNIPPET FINDINGS\n",
    "import numpy as np\n",
    "\n",
    "# Get the column names after one-hot encoding\n",
    "one_hot_feature_names = preprocessor.transformers_[1][1]['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine the names of numeric and one-hot encoded features\n",
    "feature_names = list(numeric_features) + list(one_hot_feature_names)\n",
    "\n",
    "# Convert preprocessed data to DataFrame for visualization\n",
    "X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "\n",
    "# Ensure that the indices match\n",
    "missing_indices = set(X_train_df.columns) - set(feature_names)\n",
    "for missing_index in missing_indices:\n",
    "    X_train_df[missing_index] = np.zeros((len(X_train_df), 1))\n",
    "\n",
    "# Plot histograms for numerical features\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.histplot(X_train_df[feature], kde=True)\n",
    "    plt.title(f'{feature} Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot bar plots for categorical features\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.countplot(x=X_train_df[feature])\n",
    "    plt.title(f'{feature} Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf48fc",
   "metadata": {},
   "source": [
    "<h4> Explanation of the visualization above </h4>\n",
    "<p> </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99d90dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9538\n",
      "Confusion Matrix for Logistic Regression:\n",
      " [[   0  462]\n",
      " [   0 9538]]\n",
      "Classification Report for Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Lung Cancer       0.00      0.00      0.00       462\n",
      "        None       0.95      1.00      0.98      9538\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.48      0.50      0.49     10000\n",
      "weighted avg       0.91      0.95      0.93     10000\n",
      "\n",
      "Decision Tree Accuracy: 0.9398\n",
      "Confusion Matrix for Decision Tree:\n",
      " [[   3  459]\n",
      " [ 143 9395]]\n",
      "Classification Report for Decision Tree:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Lung Cancer       0.02      0.01      0.01       462\n",
      "        None       0.95      0.99      0.97      9538\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.49      0.50      0.49     10000\n",
      "weighted avg       0.91      0.94      0.92     10000\n",
      "\n",
      "Random Forest Accuracy: 0.9528\n",
      "Confusion Matrix for Random Forest:\n",
      " [[   0  462]\n",
      " [  10 9528]]\n",
      "Classification Report for Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Lung Cancer       0.00      0.00      0.00       462\n",
      "        None       0.95      1.00      0.98      9538\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.48      0.50      0.49     10000\n",
      "weighted avg       0.91      0.95      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries to run the learning models \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate the models and print results \n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    results[name] = accuracy\n",
    "    print(f'{name} Accuracy: {accuracy}')\n",
    "    print(f'Confusion Matrix for {name}:\\n', confusion_matrix(y_test, predictions))\n",
    "    print(f'Classification Report for {name}:\\n', classification_report(y_test, predictions, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c2a25",
   "metadata": {},
   "source": [
    "<h3>Explanation of models and their respective outcomes </h3>\n",
    "<p> Ref.[5, 6 & 7] </p>\n",
    "<p> Generally, the code above imports the required libraries for the model's, then it initilizes the respective models, and lastly trains and evaluate the models outputting the fidings, such as Accuracy, Precision Matrix, and Classification Report. </p>\n",
    "<p> The chosen models were: Logistic Regression, Decision Tree, and Random Forest, see details below. </p>\n",
    "<p> <b>Logistic Regression: </b> </p> \n",
    "<p> <b>Decision Tree: </b> </p> \n",
    "<p> <b>Random Forest: </b> </p> \n",
    "<p> <b> </b> </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7bdd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library to show model performance\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# Importing library to plot the result found\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plotting the accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(results.keys(), results.values())\n",
    "plt.title('Comparison of Model Accuracies')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Making a prediction sample\n",
    "sample = X_test[0].reshape(1, -1) \n",
    "for name, model in models.items():\n",
    "    prediction = model.predict(sample)\n",
    "    print(f'{name} prediction for the first sample: {prediction}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2030ea5",
   "metadata": {},
   "source": [
    "<h3> Explanation of the code provided above to perform the models </h3> \n",
    "<p> </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a204f5",
   "metadata": {},
   "source": [
    "<h4> Making use of Dashboards </h4>\n",
    "<p> For creating the dashboards in this project, we made use of some tools like Plotly. We designed an iteractive dashboard to display user input data, visualize trands in meals, exercise, and lifestyle habits, and present predictive analytics for disease risk assessment. The following dashboards provide users with a comprehensive overview of their health status, and a proactive management for the insights. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be87cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ef54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e399574",
   "metadata": {},
   "source": [
    "<h4> Code explanation and visualization overview </h4>\n",
    "<p>  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb524fa",
   "metadata": {},
   "source": [
    "<h4> Machine Learning - Classification Model </h4>\n",
    "<p> The chosen model is <b> Random Forest Classifier</b>, due to its accuracy, and ability to handle both numerical and categorical features in an efficient way. The model was decided taking into consideration that our project involves predicting the risk of developing certain diseases based on lifestyle habits and nutritional information. Therefore the project will be based on the predictive modelling. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2d388",
   "metadata": {},
   "source": [
    "<h4> Addition of a dataset, it will be used for the disease prediction along with user gathered data </h4>\n",
    "<p> The chosen extra dataset is    which will be merged to our current 'user_data.csv', for then predicting what will be required. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6524831",
   "metadata": {},
   "source": [
    "<h4> Explanation of the code above - classification model performance </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f95777",
   "metadata": {},
   "source": [
    "<h2>References </h2>\n",
    "<p> 1. Data Collection and Data Preprocessing in Machine Learning with Python. Turing.com. Available from: <a> https://www.turing.com/kb/how-data-collection-and-data-preprocessing-in-python-help-in-machine-learning </a> [accessed 28 April 2024]. </p>\n",
    "<p> 2. R., J. (2023). 1. Data Collection using python. Medium.com. Available from: <a> https://medium.com/@renrihui8415/1-data-collection-using-python-a31a7417d422 </a> [accessed 28 April 2024].</p>\n",
    "<p> 3.  Madendere Barın, G. (2024). Effective Data Cleaning Strategies in Python for Exploratory Data Analysis (EDA). Medium.com. Available from: <a> https://medium.com/@gozdemadendere/effective-data-cleaning-strategies-in-python-for-exploratory-data-analysis-eda-c9025d5f35e4 </a> [accessed 28 April 2024]. </p>\n",
    "<p> 4. Chaitanya, N. (2023). Critical steps to training and evaluating AI and ML models. columbus.com. Available from: <a> https://www.columbusglobal.com/en/blog/critical-steps-to-training-and-evaluating-ai-and-ml-models </a> [accessed 30 April 2024]. </p>\n",
    "<p> 5. What is Logistic Regression? IBM.  Available from: <a> https://www.ibm.com/topics/logistic-regression </a> [accessed 30 April 2024]. </p>\n",
    "<p> 6. Decision Trees for Decision Making. Harvard Bussiness Review. Available from: <a> https://hbr.org/1964/07/decision-trees-for-decision-making </a> [accessed 30 April 2024]. </p>\n",
    "<p> 7. Random Forest. h2o.ai. Available from: <a> https://h2o.ai/wiki/random-forest/ </a> [accessed 30 April 2024]. </p>\n",
    "<p> 8. </p>\n",
    "<p> 9. </p>\n",
    "<p> 10. </p>\n",
    "<p> 11. </p>\n",
    "<p> 12. </p>\n",
    "<p> 13. </p>\n",
    "<p> 14. </p>\n",
    "<p> 15. </p>\n",
    "<p> 16. </p>\n",
    "<p> 17. </p>\n",
    "<p> 18. </p>\n",
    "<p> 19. </p>\n",
    "<p> 20. </p>\n",
    "<p> 21. </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
