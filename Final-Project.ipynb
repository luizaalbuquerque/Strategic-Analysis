{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be2966ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9538\n",
      "Confusion Matrix for Logistic Regression:\n",
      " [[   0  462]\n",
      " [   0 9538]]\n",
      "Classification Report for Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Lung Cancer       0.00      0.00      0.00       462\n",
      "        None       0.95      1.00      0.98      9538\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.48      0.50      0.49     10000\n",
      "weighted avg       0.91      0.95      0.93     10000\n",
      "\n",
      "Decision Tree Accuracy: 0.8872\n",
      "Confusion Matrix for Decision Tree:\n",
      " [[  28  434]\n",
      " [ 694 8844]]\n",
      "Classification Report for Decision Tree:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Lung Cancer       0.04      0.06      0.05       462\n",
      "        None       0.95      0.93      0.94      9538\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.50      0.49      0.49     10000\n",
      "weighted avg       0.91      0.89      0.90     10000\n",
      "\n",
      "Random Forest Accuracy: 0.9538\n",
      "Confusion Matrix for Random Forest:\n",
      " [[   0  462]\n",
      " [   0 9538]]\n",
      "Classification Report for Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Lung Cancer       0.00      0.00      0.00       462\n",
      "        None       0.95      1.00      0.98      9538\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.48      0.50      0.49     10000\n",
      "weighted avg       0.91      0.95      0.93     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression prediction for the first sample: ['None']\n",
      "Decision Tree prediction for the first sample: ['None']\n",
      "Random Forest prediction for the first sample: ['None']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the data\n",
    "df = pd.read_csv('Health_Data_50k_English.csv')\n",
    "\n",
    "# Removing columns not used in the model\n",
    "df.drop(['Full Name'], axis=1, inplace=True)\n",
    "\n",
    "# Data preparation\n",
    "# Encoding categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Gender', 'Alcohol Consumption', 'Physical Activity', 'Diet', 'Obesity']\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X = df.drop('Diagnosed Disease', axis=1)\n",
    "y = df['Diagnosed Disease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalising the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate the models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    results[name] = accuracy\n",
    "    print(f'{name} Accuracy: {accuracy}')\n",
    "    print(f'Confusion Matrix for {name}:\\n', confusion_matrix(y_test, predictions))\n",
    "    print(f'Classification Report for {name}:\\n', classification_report(y_test, predictions, zero_division=0))\n",
    "\n",
    "# Plotting the accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(results.keys(), results.values())\n",
    "plt.title('Comparison of Model Accuracies')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Making a prediction sample\n",
    "sample = X_test[0].reshape(1, -1)  # Reshape for a single sample\n",
    "for name, model in models.items():\n",
    "    prediction = model.predict(sample)\n",
    "    print(f'{name} prediction for the first sample: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "411ec0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Loading the data\n",
    "df = pd.read_csv('Health_Data_50k_English.csv')\n",
    "\n",
    "# Removing columns not used in the model\n",
    "df.drop(['Full Name'], axis=1, inplace=True)\n",
    "\n",
    "# Data preparation\n",
    "# Encoding categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Gender', 'Alcohol Consumption', 'Physical Activity', 'Diet', 'Obesity']\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X = df.drop('Diagnosed Disease', axis=1)\n",
    "y = df['Diagnosed Disease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalising the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the models and scaler to a pickle file\n",
    "with open('trained_models_scaler.pkl', 'wb') as file:\n",
    "    pickle.dump((models, scaler), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50714a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ccb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f46b969f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in new dataset: ['Full_Name', 'Date_of_Birth', 'Gender', 'Email', 'Phone_Number', 'Address', 'Existing_Medical_Conditions', 'Current_Medications', 'Past_Surgeries', 'Diet', 'Smoke', 'Alcohol', 'Physical_Exercise', 'Family_Health_Issues']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One or more required columns are missing from the new dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Ensure these columns exist in the dataset\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(col \u001b[38;5;129;01min\u001b[39;00m new_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m required_columns):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more required columns are missing from the new dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Continue processing as before\u001b[39;00m\n\u001b[1;32m     29\u001b[0m new_df_numeric \u001b[38;5;241m=\u001b[39m new_df[required_columns]\n",
      "\u001b[0;31mValueError\u001b[0m: One or more required columns are missing from the new dataset."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the models and scaler from the pickle file\n",
    "with open('trained_models_scaler.pkl', 'rb') as file:\n",
    "    models, scaler = pickle.load(file)\n",
    "\n",
    "# Load the new dataset\n",
    "new_df = pd.read_csv('Detailed_Patient_Health_Data.csv')\n",
    "\n",
    "# Verify and adjust column names based on the actual columns available in your new dataset\n",
    "print(\"Columns in new dataset:\", new_df.columns.tolist())\n",
    "\n",
    "# Adjust this list based on the actual data\n",
    "required_columns = [\n",
    "    'Age', 'Gender', 'Alcohol Consumption', 'Physical Activity', 'Diet', 'Obesity',\n",
    "    'Diastolic Blood Pressure (mmHg)', 'HDL Cholesterol (mg/dL)', 'LDL Cholesterol (mg/dL)', \n",
    "    'Triglycerides (mg/dL)', 'Systolic Blood Pressure (mmHg)'\n",
    "]\n",
    "\n",
    "# Ensure these columns exist in the dataset\n",
    "if not all(col in new_df.columns for col in required_columns):\n",
    "    raise ValueError(\"One or more required columns are missing from the new dataset.\")\n",
    "\n",
    "# Continue processing as before\n",
    "new_df_numeric = new_df[required_columns]\n",
    "new_df_scaled = scaler.transform(new_df_numeric)\n",
    "random_index = random.randint(0, len(new_df) - 1)\n",
    "random_user_scaled = new_df_scaled[random_index].reshape(1, -1)\n",
    "\n",
    "for name, model in models.items():\n",
    "    prediction = model.predict(random_user_scaled)\n",
    "    print(f'{name} prediction for the randomly selected user: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "110ea8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'Logistic Regression': LogisticRegression(), 'Decision Tree': DecisionTreeClassifier(), 'Random Forest': RandomForestClassifier()}, StandardScaler())\n",
      "({'Logistic Regression': LogisticRegression(), 'Decision Tree': DecisionTreeClassifier(), 'Random Forest': RandomForestClassifier()}, StandardScaler())\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Caminho para o arquivo .pkl\n",
    "file_path = 'trained_models_scaler.pkl'\n",
    "\n",
    "# Carregando o arquivo\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Imprimindo o conteúdo do objeto\n",
    "print(data)\n",
    "\n",
    "# Se o objeto for complexo ou um DataFrame, você pode querer imprimir de forma mais estruturada\n",
    "if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "        print(f\"Chave: {key}, Valor: {value}\")\n",
    "elif hasattr(data, 'head'):  # Checa se é um DataFrame do pandas\n",
    "    print(data.head())  # Imprime as primeiras linhas do DataFrame\n",
    "else:\n",
    "    print(data)  # Imprime o objeto diretamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d13ae0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in new dataset: ['Full_Name', 'Date_of_Birth', 'Gender', 'Email', 'Phone_Number', 'Address', 'Existing_Medical_Conditions', 'Current_Medications', 'Past_Surgeries', 'Diet', 'Smoke', 'Alcohol', 'Physical_Exercise', 'Family_Health_Issues']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edgardpacheco/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- Alcohol\n",
      "- Physical_Exercise\n",
      "- Smoke\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Age\n",
      "- Alcohol Consumption\n",
      "- Diastolic Blood Pressure (mmHg)\n",
      "- HDL Cholesterol (mg/dL)\n",
      "- LDL Cholesterol (mg/dL)\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 5 features, but StandardScaler is expecting 13 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Prepare the data using only the available numeric columns\u001b[39;00m\n\u001b[1;32m     31\u001b[0m new_df_numeric \u001b[38;5;241m=\u001b[39m new_df[required_columns]\n\u001b[0;32m---> 32\u001b[0m new_df_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df_numeric\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Normalize data\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Select a random user from the new dataset\u001b[39;00m\n\u001b[1;32m     35\u001b[0m random_index \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(new_df) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:973\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    970\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    972\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m--> 973\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 5 features, but StandardScaler is expecting 13 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the models and scaler from the pickle file\n",
    "with open('trained_models_scaler.pkl', 'rb') as file:\n",
    "    models, scaler = pickle.load(file)\n",
    "\n",
    "# Load the new dataset\n",
    "new_df = pd.read_csv('Detailed_Patient_Health_Data.csv')\n",
    "\n",
    "# Check and verify the column names\n",
    "print(\"Columns in new dataset:\", new_df.columns.tolist())\n",
    "\n",
    "# Adjust this based on the actual data columns available\n",
    "required_columns = ['Gender', 'Diet', 'Smoke', 'Alcohol', 'Physical_Exercise']  # Example of possible available columns\n",
    "\n",
    "# Ensure these columns exist in the dataset\n",
    "if not all(col in new_df.columns for col in required_columns):\n",
    "    raise ValueError(\"One or more required columns are missing from the new dataset.\")\n",
    "\n",
    "# Adjust label encoding for categorical columns that are available and used by models\n",
    "for col in required_columns:\n",
    "    if col in new_df.columns:\n",
    "        le = LabelEncoder()\n",
    "        new_df[col] = le.fit_transform(new_df[col])\n",
    "\n",
    "# Prepare the data using only the available numeric columns\n",
    "new_df_numeric = new_df[required_columns]\n",
    "new_df_scaled = scaler.transform(new_df_numeric)  # Normalize data\n",
    "\n",
    "# Select a random user from the new dataset\n",
    "random_index = random.randint(0, len(new_df) - 1)\n",
    "random_user_scaled = new_df_scaled[random_index].reshape(1, -1)\n",
    "\n",
    "# Make predictions with each of the trained models\n",
    "for name, model in models.items():\n",
    "    prediction = model.predict(random_user_scaled)\n",
    "    print(f'{name} prediction for the randomly selected user: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5cff53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns available: ['Full_Name', 'Date_of_Birth', 'Gender', 'Email', 'Phone_Number', 'Address', 'Existing_Medical_Conditions', 'Current_Medications', 'Past_Surgeries', 'Diet', 'Smoke', 'Alcohol', 'Physical_Exercise', 'Family_Health_Issues']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edgardpacheco/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Alcohol Consumption\n",
      "- Diastolic Blood Pressure (mmHg)\n",
      "- HDL Cholesterol (mg/dL)\n",
      "- LDL Cholesterol (mg/dL)\n",
      "- Obesity\n",
      "- ...\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 3 features, but StandardScaler is expecting 13 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Prepare data for prediction: select and scale the necessary columns\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Assuming you have a list of necessary columns from your model training\u001b[39;00m\n\u001b[1;32m     36\u001b[0m required_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiet\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Add more as per your model\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m user_data_for_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_user\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrequired_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Making predictions using the loaded models\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:973\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    970\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    972\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m--> 973\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 3 features, but StandardScaler is expecting 13 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the models and scaler from the pickle file\n",
    "with open('trained_models_scaler.pkl', 'rb') as file:\n",
    "    models, scaler = pickle.load(file)\n",
    "\n",
    "# Load the new dataset\n",
    "new_df = pd.read_csv('Detailed_Patient_Health_Data.csv')\n",
    "\n",
    "# Print the columns to check what we have\n",
    "print(\"Columns available:\", new_df.columns.tolist())\n",
    "\n",
    "# Assuming we need to transform some columns\n",
    "# Example: Encode 'Gender', 'Diet', 'Smoke', 'Alcohol', 'Physical_Exercise'\n",
    "# These need to be adapted based on actual available data and what models were trained on\n",
    "categorical_cols = ['Gender', 'Diet']  # Adapt based on your data\n",
    "for col in categorical_cols:\n",
    "    if col in new_df.columns:\n",
    "        le = LabelEncoder()\n",
    "        new_df[col] = le.fit_transform(new_df[col])\n",
    "\n",
    "# Assuming 'Age' needs to be calculated or is directly available\n",
    "# If 'Date_of_Birth' is available:\n",
    "if 'Date_of_Birth' in new_df.columns:\n",
    "    new_df['Age'] = 2023 - pd.to_datetime(new_df['Date_of_Birth']).dt.year\n",
    "\n",
    "# Select a random user\n",
    "random_user = new_df.sample(n=1)\n",
    "\n",
    "# Prepare data for prediction: select and scale the necessary columns\n",
    "# Assuming you have a list of necessary columns from your model training\n",
    "required_columns = ['Age', 'Gender', 'Diet']  # Add more as per your model\n",
    "user_data_for_prediction = scaler.transform(random_user[required_columns])\n",
    "\n",
    "# Making predictions using the loaded models\n",
    "for name, model in models.items():\n",
    "    prediction = model.predict(user_data_for_prediction)\n",
    "    print(f'{name} prediction for the selected user: {prediction}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
