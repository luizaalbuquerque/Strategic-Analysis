{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd279114",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbc0980d",
   "metadata": {},
   "source": [
    "<h1> Strategic Analysis </h1>\n",
    "<h4> Strategic Business Information Technology & Problem Solving for Industry (Capstone Project) </h4>\n",
    "<p> Lecturers: Ken Healy & Muhammad Iqbal </p>\n",
    "<p> Students: Luiza Cavalcanti Albuquerque Brayner (2020309) & Edgard Pacheco (2020332)  </p>\n",
    "\n",
    "<b>Solutions, Findings, and Demonstrations. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f549c5",
   "metadata": {},
   "source": [
    "<h2> Introduction </h2>\n",
    "<b> In this part of the project, we will be providing a comprehensive solution for a personalized health track and disease risk prediction application. We will demonstrate our data collection, cleanning and visualization  of the fidings, AI models to be used, providing a clear overview of our innovative approach. In this project, we will focus over the prediction of a specific risk of developing cardiovascular diseases, by making use of predictive models, we will make a comparison of some model's performance in making such prediction, and justification of the selection of the final model. </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f538356",
   "metadata": {},
   "source": [
    "<h3> Data Gathering Process </h3>\n",
    "<p> User data, will be gathered by a dataset that is previosly done, with some information that would be gathered from the user's input. All user data currently being analysed is fictional, and in the future could be altered to appropriately match the application goals, which offers options for manual input, and therefore provide a personalized response to better user experience. There will also be an upfront form that will be required for user input of personal and health data, that is where the data will come from to do the first prediction. But for testing purposes, we will be training the models over a fictional dataset, mainly created by a snipped of code below. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a816dbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Smoking Years</th>\n",
       "      <th>Alcohol Consumption</th>\n",
       "      <th>Physical Activity</th>\n",
       "      <th>Diet</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Diagnosed Disease</th>\n",
       "      <th>Total Cholesterol (mg/dL)</th>\n",
       "      <th>LDL Cholesterol (mg/dL)</th>\n",
       "      <th>HDL Cholesterol (mg/dL)</th>\n",
       "      <th>Triglycerides (mg/dL)</th>\n",
       "      <th>Systolic Blood Pressure (mmHg)</th>\n",
       "      <th>Diastolic Blood Pressure (mmHg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>Female</td>\n",
       "      <td>10</td>\n",
       "      <td>Never</td>\n",
       "      <td>Intense</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>Yes</td>\n",
       "      <td>None</td>\n",
       "      <td>178</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>210</td>\n",
       "      <td>171</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>Female</td>\n",
       "      <td>8</td>\n",
       "      <td>Never</td>\n",
       "      <td>Intense</td>\n",
       "      <td>Unhealthy</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>227</td>\n",
       "      <td>105</td>\n",
       "      <td>22</td>\n",
       "      <td>245</td>\n",
       "      <td>143</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>Other</td>\n",
       "      <td>5</td>\n",
       "      <td>Never</td>\n",
       "      <td>None</td>\n",
       "      <td>Balanced</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>170</td>\n",
       "      <td>64</td>\n",
       "      <td>43</td>\n",
       "      <td>258</td>\n",
       "      <td>175</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>Never</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Vegan</td>\n",
       "      <td>Yes</td>\n",
       "      <td>None</td>\n",
       "      <td>206</td>\n",
       "      <td>79</td>\n",
       "      <td>46</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Other</td>\n",
       "      <td>18</td>\n",
       "      <td>Never</td>\n",
       "      <td>Light</td>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "      <td>216</td>\n",
       "      <td>131</td>\n",
       "      <td>78</td>\n",
       "      <td>208</td>\n",
       "      <td>178</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender  Smoking Years Alcohol Consumption Physical Activity  \\\n",
       "0   31  Female             10               Never           Intense   \n",
       "1   36  Female              8               Never           Intense   \n",
       "2   24   Other              5               Never              None   \n",
       "3   41    Male              1               Never          Moderate   \n",
       "4   28   Other             18               Never             Light   \n",
       "\n",
       "         Diet Obesity Diagnosed Disease  Total Cholesterol (mg/dL)  \\\n",
       "0   Unhealthy     Yes              None                        178   \n",
       "1   Unhealthy      No              None                        227   \n",
       "2    Balanced      No              None                        170   \n",
       "3       Vegan     Yes              None                        206   \n",
       "4  Vegetarian      No              None                        216   \n",
       "\n",
       "   LDL Cholesterol (mg/dL)  HDL Cholesterol (mg/dL)  Triglycerides (mg/dL)  \\\n",
       "0                       62                       62                    210   \n",
       "1                      105                       22                    245   \n",
       "2                       64                       43                    258   \n",
       "3                       79                       46                    117   \n",
       "4                      131                       78                    208   \n",
       "\n",
       "   Systolic Blood Pressure (mmHg)  Diastolic Blood Pressure (mmHg)  \n",
       "0                             171                               93  \n",
       "1                             143                               89  \n",
       "2                             175                              115  \n",
       "3                             117                               90  \n",
       "4                             178                               71  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading the data from a previous dataset \n",
    "df = pd.read_csv('Health_Data_50k_English.csv')\n",
    "\n",
    "# Data preparation\n",
    "# Removing columns not used in the model - making the dataset personalized \n",
    "df.drop(['Full Name'], axis=1, inplace=True)\n",
    "\n",
    "# Printing first few rows of the dataset to be analysed \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d63765",
   "metadata": {},
   "source": [
    "<h3> Explanation of the code provided above to gather user data </h3>\n",
    "<p> <b> 1. Collecting user data:</b> The main idea is to output a form where the user can input it's own health data and personal data, but for testing purposes as said above, we will be creating such data.</p>\n",
    "<p> <b> 2. Storing the data:</b> The data is gathered from a previous dataset and then adapted to our own usage, excluding columns that will not be in usage for the analyses. The data is then stored temporarely in a variable. </p>\n",
    "<p> <b> 3. Writting to CSV:</b> In the original idea, the code will also write the collected user data into a CSV file called \"user_data.csv\". The file is already created, and it writes on the following available roll. But following the test purpose, this step of the data gathering will be delayed for the implementation part. </p>\n",
    "<p> <b> 4. Confirmation:</b> Again, focusing over the original idea, after the data is added to the CSV file, then there will be a confirmation message, which will indicate that the data has been submited. But for testing purposes, we double check the dataframe, by printting the first few rows and analysing the output. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c891457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataset: (50000, 14)\n",
      "Missing values:\n",
      " Age                                0\n",
      "Gender                             0\n",
      "Smoking Years                      0\n",
      "Alcohol Consumption                0\n",
      "Physical Activity                  0\n",
      "Diet                               0\n",
      "Obesity                            0\n",
      "Diagnosed Disease                  0\n",
      "Total Cholesterol (mg/dL)          0\n",
      "LDL Cholesterol (mg/dL)            0\n",
      "HDL Cholesterol (mg/dL)            0\n",
      "Triglycerides (mg/dL)              0\n",
      "Systolic Blood Pressure (mmHg)     0\n",
      "Diastolic Blood Pressure (mmHg)    0\n",
      "dtype: int64\n",
      "Data types:\n",
      " Age                                 int64\n",
      "Gender                             object\n",
      "Smoking Years                       int64\n",
      "Alcohol Consumption                object\n",
      "Physical Activity                  object\n",
      "Diet                               object\n",
      "Obesity                            object\n",
      "Diagnosed Disease                  object\n",
      "Total Cholesterol (mg/dL)           int64\n",
      "LDL Cholesterol (mg/dL)             int64\n",
      "HDL Cholesterol (mg/dL)             int64\n",
      "Triglycerides (mg/dL)               int64\n",
      "Systolic Blood Pressure (mmHg)      int64\n",
      "Diastolic Blood Pressure (mmHg)     int64\n",
      "dtype: object\n",
      "Statistics (summary):\n",
      "                 Age  Smoking Years  Total Cholesterol (mg/dL)  \\\n",
      "count  50000.000000   50000.000000               50000.000000   \n",
      "mean      48.657020      19.522300                 199.332760   \n",
      "std       17.895249      11.510127                  28.879299   \n",
      "min       18.000000       0.000000                 150.000000   \n",
      "25%       33.000000      10.000000                 174.000000   \n",
      "50%       49.000000      20.000000                 199.000000   \n",
      "75%       64.000000      30.000000                 224.000000   \n",
      "max       79.000000      39.000000                 249.000000   \n",
      "\n",
      "       LDL Cholesterol (mg/dL)  HDL Cholesterol (mg/dL)  \\\n",
      "count             50000.000000              50000.00000   \n",
      "mean                 99.379560                 49.48520   \n",
      "std                  28.913989                 17.30154   \n",
      "min                  50.000000                 20.00000   \n",
      "25%                  74.000000                 34.00000   \n",
      "50%                  99.000000                 49.00000   \n",
      "75%                 125.000000                 64.00000   \n",
      "max                 149.000000                 79.00000   \n",
      "\n",
      "       Triglycerides (mg/dL)  Systolic Blood Pressure (mmHg)  \\\n",
      "count           50000.000000                    50000.000000   \n",
      "mean              199.365660                      144.528000   \n",
      "std                58.036432                       20.263532   \n",
      "min               100.000000                      110.000000   \n",
      "25%               149.000000                      127.000000   \n",
      "50%               199.000000                      144.000000   \n",
      "75%               250.000000                      162.000000   \n",
      "max               299.000000                      179.000000   \n",
      "\n",
      "       Diastolic Blood Pressure (mmHg)  \n",
      "count                     50000.000000  \n",
      "mean                         94.417000  \n",
      "std                          14.422579  \n",
      "min                          70.000000  \n",
      "25%                          82.000000  \n",
      "50%                          94.000000  \n",
      "75%                         107.000000  \n",
      "max                         119.000000  \n",
      "Number of duplicated rows: 0\n",
      "Number of outliers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/19n82dfx2jdcfq3h1tg_4jzh0000gn/T/ipykernel_5174/1861733410.py:34: FutureWarning: Automatic reindexing on DataFrame vs Series comparisons is deprecated and will raise ValueError in a future version.  Do `left, right = left.align(right, axis=1, copy=False)` before e.g. `left == right`\n",
      "  outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAGDCAYAAABdtKgRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYOklEQVR4nO3df7RdZX3n8ffHAKLFokjEQKKhkq4KVqlzRap2iiAtQZdgtYpSQWsntTZOC1M6UZyOuqYzTHUsUhFWllVBqZT6M9ZUFPBHZyrCDUo0ZZAYi0QiBGcKIioGv/PH2ZHDnXPvPeFy7knu836tdde9Z+9n7/0cWCvnffY+P1JVSJKktjxs3BOQJEnzzwCQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIGokkL0pyS5K7k/zKPB3zmCRb+25vSnLMfBxb2tMYANJuLskrkkx2D6TbkvxDkufMw3EryWFz2MXbgdVVtV9VfWXA/pPkrCQ3Jflhkm8nOSfJwx+qOVbVEVX1+Qc3fWlhMwCk3ViSM4Fzgf8KHAQ8AXg3cNIYpzWsJwKbZlh/HrAKOA14FLASOBa4bPRTm1mSvcY9B2nUDABpN5Vkf+CtwB9W1Uer6gdV9ZOq+mRVndWNeXiSc5Pc2v2cu/MZdJJXJfmfU/b5s2fMSd6f5Pwkn0ry/SRfTvKkbt0Xu02u7848vGzA/B6W5E1Jbk5ye5KLk+zfzeluYFG3/TcHbLsCeB1walV9qap2VNUm4MXACUmO7cZ9Psnv9W33s/s05Bz/Jcnz+ua7Jsk3k3wvyWVJDujWLe/+27wmybeBq5Lsm+SD3dh/TXJtkoOG+p8n7QEMAGn39avAvsDHZhhzNnA0cCTwNOAo4E27cIyXA28BHgNsBv4coKr+bbf+ad0p/L8dsO2rup/nAr8A7Ae8q6p+XFX79W3/pAHbHgdsrapr+hdW1S3A1cDxs018yDn2+/fAycCvAwcD/xc4f8qYXweeDPwmcDqwP7AMeCzwWuCHs81L2lMYANLu67HAHVW1Y4YxpwJvrarbq2o7vQfzV+7CMT5aVdd0x7iEXkgM61TgHVW1paruBt4AnDLk6fMDgW3TrNvWrX+o/T5wdlVtraofA28GXjJlvm/uzrT8EPgJvf8Hh1XVfVW1oaruGsG8pLEwAKTd1/eAA2d5QD0YuLnv9s3dsmF9t+/ve+g9ix/WoGPvRe+1CrO5A1gyzbol3fqH2hOBj3Wn8/8VuAG4jwfO95a+vz8AXA5c2l1e+Yske49gXtJYGADS7utLwI/onbaezq30Hth2ekK3DOAHwCN3rkjy+Id4foOOvQO4bYhtrwKWJTmqf2GSZfQuaVzZLXrAfQDmch9uAVZW1aP7fvatqu/0jfnZ16N2r7d4S1UdDjwLeAG9FyxKC4IBIO2mqupO4M+A85OcnOSRSfZOsjLJX3TDPgS8KcniJAd24z/YrbseOCLJkUn2pXfKe1fcRu/a/nQ+BJyR5NAk+9F7p8LfznLJYud9+wZwIXBJkqOTLEpyBPAR4IqquqIb+lXgt7r7fhjwml2cY78LgT9P8kSA7r/ZtO+mSPLcJL+cZBFwF71LAvcNeSxpt2cASLuxqnoHcCa9F/Ztp/csdjXw8W7IfwEmgY3A14DrumU7H2TfClwB3AQ84B0BQ3gzcFF3yvylA9a/l95p8i8C36J3tuL1u7D/1cB76AXL3cCngc/TeyfATn8J3Evvgf4ieq9T2JU59nsnsA74TJLv03ux4TNnGP944MP0HvxvAL7A/XEl7fFSVbOPkiRJC4pnACRJapABIElSgwwASZIaZABIktQgA0CSpAY19Y1XBx54YC1fvnzc05AkaV5s2LDhjqpaPGhdUwGwfPlyJicnxz0NSZLmRZKbp1vnJQBJkhpkAEiS1CADQJKkBhkAkiQ1yACQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIktQgA0CSpAYZAJIkNcgAkCSpQQaAJEkNMgAkSWqQASBJUoMMAEmSGmQASJLUIANAkqQGGQCSJDXIAJAkqUEGgCRJDTIAJElqkAEgSVKDDABJkhpkAEiS1CADQJKkBhkAkiQ1yACQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIktQgA0CSpAYZAJIkNcgAkCSpQQaAJEkNMgAkSWrQWAMgyQlJbkyyOcmaAeuT5Lxu/cYkT5+yflGSryT5+/mbtSRJe76xBUCSRcD5wErgcODlSQ6fMmwlsKL7WQVcMGX9HwE3jHiqkiQtOOM8A3AUsLmqtlTVvcClwElTxpwEXFw9VwOPTrIEIMlS4PnAe+Zz0pIkLQTjDIBDgFv6bm/tlg075lzgT4GfznSQJKuSTCaZ3L59+5wmLEnSQjHOAMiAZTXMmCQvAG6vqg2zHaSq1lbVRFVNLF68+MHMU5KkBWecAbAVWNZ3eylw65Bjng28MMm/0Lt0cGySD45uqpIkLSzjDIBrgRVJDk2yD3AKsG7KmHXAad27AY4G7qyqbVX1hqpaWlXLu+2uqqrfmdfZS5K0B9trXAeuqh1JVgOXA4uA91bVpiSv7dZfCKwHTgQ2A/cArx7XfCVJWkhSNfWy+8I1MTFRk5OT456GJEnzIsmGqpoYtM5PApQkqUEGgCRJDTIAJElqkAEgSVKDDABJkhpkAEiS1CADQJKkBhkAkiQ1yACQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIktQgA0CSpAYZAJIkNcgAkCSpQQaAJEkNMgAkSWqQASBJUoMMAEmSGmQASJLUIANAkqQGGQCSJDXIAJAkqUEGgCRJDTIAJElqkAEgSVKDDABJkhpkAEiS1CADQJKkBhkAkiQ1yACQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIktQgA0CSpAYZAJIkNcgAkCSpQQaAJEkNMgAkSWqQASBJUoMMAEmSGmQASJLUIANAkqQGGQCSJDXIAJAkqUEGgCRJDRprACQ5IcmNSTYnWTNgfZKc163fmOTp3fJlST6X5IYkm5L80fzPXpKkPdfYAiDJIuB8YCVwOPDyJIdPGbYSWNH9rAIu6JbvAP5DVT0ZOBr4wwHbSpKkaYzzDMBRwOaq2lJV9wKXAidNGXMScHH1XA08OsmSqtpWVdcBVNX3gRuAQ+Zz8pIk7cnGGQCHALf03d7K//8gPuuYJMuBXwG+POggSVYlmUwyuX379rnOWZKkBWGcAZABy2pXxiTZD/gI8MdVddegg1TV2qqaqKqJxYsXP+jJSpK0kIwzALYCy/puLwVuHXZMkr3pPfhfUlUfHeE8JUlacMYZANcCK5IcmmQf4BRg3ZQx64DTuncDHA3cWVXbkgT4a+CGqnrH/E5bkqQ9317jOnBV7UiyGrgcWAS8t6o2JXltt/5CYD1wIrAZuAd4dbf5s4FXAl9L8tVu2Rurav083gVJkvZYqZp62X3hmpiYqMnJyXFPQ5KkeZFkQ1VNDFrnJwFKktQgA0CSpAYZAJIkNcgAkCSpQQaAJEkNMgAkSWqQASBJUoMMAEmSGmQASJLUIANAkqQGGQCSJDXIAJAkqUEGgCRJDTIAJElqkAEgSVKDDABJkhpkAEiS1CADQJKkBhkAkiQ1yACQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIktQgA0CSpAYZAJIkNcgAkCSpQQaAJEkNMgAkSWqQASBJUoMMAEmSGmQASJLUIANAkqQGGQCSJDXIAJAkqUEGgCRJDTIAJElqkAEgSVKDDABJkhpkAEiS1KChAiDJs4dZJkmS9gzDngH4qyGXSZKkPcBeM61M8qvAs4DFSc7sW/XzwKJRTkySJI3OjAEA7APs1417VN/yu4CXjGpSkiRptGYMgKr6AvCFJO+vqpvnaU6SJGnEZjsDsNPDk6wFlvdvU1XHjmJSkiRptIYNgL8DLgTeA9w3uulIkqT5MGwA7KiqC0Y6E0mSNG+GfRvgJ5O8LsmSJAfs/BnpzCRJ0sgMGwCnA2cB/wRs6H4m53rwJCckuTHJ5iRrBqxPkvO69RuTPH3YbSVJ0vSGugRQVYc+1AdOsgg4Hzge2Apcm2RdVf1z37CVwIru55nABcAzh9xWkiRNY6gASHLaoOVVdfEcjn0UsLmqtnTHuBQ4Ceh/ED8JuLiqCrg6yaOTLKH3boTZtpUkSdMY9kWAz+j7e1/gOOA6YC4BcAhwS9/trfSe5c825pAht5UkSdMY9hLA6/tvJ9kf+MAcj51BhxpyzDDb9naQrAJWATzhCU/YlflJkrRgPdivA76H3nX5udgKLOu7vRS4dcgxw2wLQFWtraqJqppYvHjxHKcsSdLCMOxrAD7J/c+wFwFPBi6b47GvBVYkORT4DnAK8IopY9YBq7tr/M8E7qyqbUm2D7GtJEmaxrCvAXh73987gJurautcDlxVO5KsBi6nFxXvrapNSV7brb8QWA+cCGymd9bh1TNtO5f5SJLUkvReYD/EwOQg7n8x4DVVdfvIZjUiExMTNTk5548vkCRpj5BkQ1VNDFo31GsAkrwUuAb4beClwJeT+HXAkiTtoYa9BHA28Iydz/qTLAauAD48qolJkqTRGfZdAA+bcsr/e7uwrSRJ2s0Mewbg00kuBz7U3X4ZvRfoSZKkPdCMAZDkMOCgqjoryW8Bz6H3ITxfAi6Zh/lJkqQRmO00/rnA9wGq6qNVdWZVnUHv2f+5o52aJEkaldkCYHlVbZy6sKom6X0hjyRJ2gPNFgD7zrDuEQ/lRCRJ0vyZLQCuTfLvpi5M8hpgw2imJEmSRm22dwH8MfCxJKdy/wP+BLAP8KIRzkuSJI3QjAFQVbcBz0ryXOAp3eJPVdVVI5+ZJEkamaE+B6CqPgd8bsRzkSRJ88RP85MkqUEGgCRJDTIAJElqkAEgSVKDDABJkhpkAEiS1CADQJKkBhkAkiQ1yACQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIktQgA0CSpAYZAJIkNcgAkCSpQQaAJEkNMgAkSWqQASBJUoMMAEmSGmQASJLUIANAkqQGGQCSJDXIAJAkqUEGgCRJDTIAJElqkAEgSVKDDABJkhpkAEiS1CADQJKkBhkAkiQ1yACQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIktQgA0CSpAaNJQCSHJDks0lu6n4/ZppxJyS5McnmJGv6lr8tyf9OsjHJx5I8et4mL0nSAjCuMwBrgCuragVwZXf7AZIsAs4HVgKHAy9Pcni3+rPAU6rqqcA3gDfMy6wlSVogxhUAJwEXdX9fBJw8YMxRwOaq2lJV9wKXdttRVZ+pqh3duKuBpaOdriRJC8u4AuCgqtoG0P1+3IAxhwC39N3e2i2b6neBf5juQElWJZlMMrl9+/Y5TFmSpIVjr1HtOMkVwOMHrDp72F0MWFZTjnE2sAO4ZLqdVNVaYC3AxMRETTdOkqSWjCwAqup5061LcluSJVW1LckS4PYBw7YCy/puLwVu7dvH6cALgOOqygd2SZJ2wbguAawDTu/+Ph34xIAx1wIrkhyaZB/glG47kpwA/EfghVV1zzzMV5KkBWVcAXAOcHySm4Dju9skOTjJeoDuRX6rgcuBG4DLqmpTt/27gEcBn03y1SQXzvcdkCRpTzaySwAzqarvAccNWH4rcGLf7fXA+gHjDhvpBCVJWuD8JEBJkhpkAEiS1CADQJKkBhkAkiQ1yACQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIktQgA0CSpAYZAJIkNcgAkCSpQQaAJEkNMgAkSWqQASBJUoMMAEmSGmQASJLUIANAkqQGGQCSJDXIAJAkqUEGgCRJDTIAJElqkAEgSVKDDABJkhpkAEiS1CADQJKkBhkAkiQ1yACQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIktQgA0CSpAYZAJIkNcgAkCSpQQaAJEkNMgAkSWqQASBJUoMMAEmSGmQASJLUIANAkqQGGQCSJDXIAJAkqUEGgCRJDTIAJElqkAEgSVKDDABJkhpkAEiS1KCxBECSA5J8NslN3e/HTDPuhCQ3JtmcZM2A9X+SpJIcOPpZS5K0cIzrDMAa4MqqWgFc2d1+gCSLgPOBlcDhwMuTHN63fhlwPPDteZmxJEkLyLgC4CTgou7vi4CTB4w5CthcVVuq6l7g0m67nf4S+FOgRjhPSZIWpHEFwEFVtQ2g+/24AWMOAW7pu721W0aSFwLfqarrZztQklVJJpNMbt++fe4zlyRpAdhrVDtOcgXw+AGrzh52FwOWVZJHdvv4jWF2UlVrgbUAExMTni2QJIkRBkBVPW+6dUluS7KkqrYlWQLcPmDYVmBZ3+2lwK3Ak4BDgeuT7Fx+XZKjquq7D9kdkCRpARvXJYB1wOnd36cDnxgw5lpgRZJDk+wDnAKsq6qvVdXjqmp5VS2nFwpP98FfkqThjSsAzgGOT3ITvVfynwOQ5OAk6wGqagewGrgcuAG4rKo2jWm+kiQtKCO7BDCTqvoecNyA5bcCJ/bdXg+sn2Vfyx/q+UmStND5SYCSJDXIAJAkqUEGgCRJDTIAJElqkAEgSVKDDABJkhpkAEiS1CADQJKkBhkAkiQ1yACQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIktQgA0CSpAYZAJIkNcgAkCSpQQaAJEkNMgAkSWqQASBJUoMMAEmSGmQASJLUIANAkqQGGQCSJDXIAJAkqUEGgCRJDTIAJElqkAEgSVKDDABJkhpkAEiS1CADQJKkBhkAkiQ1yACQJKlBBoAkSQ0yACRJapABIElSgwwASZIaZABIktSgVNW45zBvkmwHbh73PKQGHQjcMe5JSA16YlUtHrSiqQCQNB5JJqtqYtzzkHQ/LwFIktQgA0CSpAYZAJLmw9pxT0DSA/kaAEmSGuQZAEmSGmQASJpWkqVJPpHkpiTfTPLOJPvMss0bp9y+u/t9cJIPj3K+kobnJQBJAyUJ8GXggqp6X5JF9K7l/5+qOmuG7e6uqv2mu70Lx19UVfc9mLlLmp1nACRN51jgR1X1PoDuwfgM4HeTvC7Ju3YOTPL3SY5Jcg7wiCRfTXJJ/86SLE/y9e7vRUneluTaJBuT/H63/Jgkn0vyN8DXkvxckk8luT7J15O8bJ7uu7Tg7TXuCUjabR0BbOhfUFV3Jfk20/zbUVVrkqyuqiNn2fdrgDur6hlJHg78rySf6dYdBTylqr6V5MXArVX1fIAk+8/h/kjq4xkASdMJMOga4XTLd8VvAKcl+Sq9ywyPBVZ0666pqm91f38NeF6S/57k16rqzjkeV1LHAJA0nU3AAz6+N8nPA8uAO3ngvx/77uK+A7y+qo7sfg6tqp1nAH6wc1BVfQP4N/RC4L8l+bNdPI6kaRgAkqZzJfDIJKdB77o98D+A9wNbgCOTPCzJMnqn7Xf6SZK9Z9n35cAf7ByX5BeT/NzUQUkOBu6pqg8CbweePsf7JKnjawAkDVRVleRFwLuT/Cd6TxjWA28E7gW+Re+Z+deB6/o2XQtsTHJdVZ06ze7fAywHruvebbAdOHnAuF8G3pbkp8BPgD+Y6/2S1OPbACVJapCXACRJapABIElSgwwASZIaZABIktQgA0CSpAYZAFKjktzXfWb/pu6z9s9MMuO/Cd3n+b9iDsf6epK/S/LIGca+MMmaUcxD0v0MAKldP+w+he8I4HjgROA/z7LNcuDBPPDuPNZT6H2GwGunG1hV66rqnBHNQ1LHAJBEVd0OrAJWp2d5kn9Mcl3386xu6DnAr3XP5s+YYdxM/hE4LMkBST7efRvg1UmeCpDkVTu/aTDJ+5Ocl+SfkmxJ8pJp5nFEkmu62xuTrJjm2JI6fhKgJACqakt3CeBxwO3A8VX1o+7B9EP0vhdgDfAnVfUCgO5U/qBxAyXZC1gJfBp4C/CVqjo5ybHAxcCRAzZbAjwH+CVgHfDhAfP4K+CdVXVJkn2ARXP7ryEtfAaApH7pfu8NvCvJkcB9wC9OM37YcY/ovvkPemcA/pretwC+GKCqrkry2Gm+7vfjVfVT4J+THDTN/r8EnJ1kKfDRqrppmnGSOgaAJACS/AK9B/Hb6b0W4DbgafQuFf5oms3OGHLcD6vqyCnHy4Bxgz6b/Mf9mw3aeVX9TZIvA88HLk/ye1V11TRzkYSvAZAEJFkMXAi8q3pfELI/sK175v1K7j+l/n3gUX2bTjduGF8ETu2OfwxwR1XdNeS2D5hHFy9bquo8epcJnroL85Ca5BkAqV07T8vvDewAPgC8o1v3buAjSX4b+Bzwg275RmBHkuvpfS3wdOOG8WbgfUk2AvcAp+/CtlPnsS/wO0l+AnwXeOsu7Etqkt8GKElSg7wEIElSgwwASZIaZABIktQgA0CSpAYZAJIkNcgAkCSpQQaAJEkNMgAkSWrQ/wNsUtPXxwgNCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Gender: ['Female' 'Other' 'Male']\n",
      "Unique values in Alcohol Consumption: ['Never' 'Occasionally' 'Regularly']\n",
      "Unique values in Physical Activity: ['Intense' 'None' 'Moderate' 'Light']\n",
      "Unique values in Diet: ['Unhealthy' 'Balanced' 'Vegan' 'Vegetarian']\n",
      "Unique values in Obesity: ['Yes' 'No']\n",
      "Unique values in Diagnosed Disease: ['None' 'Lung Cancer']\n"
     ]
    }
   ],
   "source": [
    "# Import for checking plot of outliers \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dimensions\n",
    "print(\"Dimensions of the dataset:\", df.shape)\n",
    "\n",
    "# Missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Data types of each feature\n",
    "print(\"Data types:\\n\", df.dtypes)\n",
    "\n",
    "# Numerical features\n",
    "print(\"Statistics (summary):\\n\", df.describe())\n",
    "\n",
    "# Creating a new DataFrame (converting an existing object df into a DataFrame) \n",
    "# It is already a dataframe so basically it is a copy of it\n",
    "data = pd.DataFrame(df)\n",
    "\n",
    "# Get duplicated rows\n",
    "duplicated_rows = df[df.duplicated(keep=False)]\n",
    "# print(\"Duplicated rows:\")\n",
    "# print(duplicated_rows)\n",
    "\n",
    "# Count duplicated rows and output\n",
    "duplicated_counts = df.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicated rows:\", duplicated_counts)\n",
    "\n",
    "# Identify outliers\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "outliers_count = outliers.sum()\n",
    "print(\"Number of outliers:\", outliers_count)\n",
    "\n",
    "# Create a bar plot to show outliers visualization \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar([\"Outliers\"], [outliers_count], color='orange')\n",
    "plt.xlabel(\"Data Points\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of Outliers\")\n",
    "plt.show()\n",
    "\n",
    "# Check for unique values in categorical columns - data validation \n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Unique values in {column}:\", unique_values)\n",
    "\n",
    "# distribution of categorical features\n",
    "# print(\"Distribution of the feature 'Action':\\n\", df['Action'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d2782",
   "metadata": {},
   "source": [
    "<h3> Explanation of the code provided above to perform a data cleanning </h3>\n",
    "<p> <b> 1. Identifying dataset dimentions: </b> The dataset to be used is formulated by 50000 instances and 14 features/attributes. Regarding the dimensions of the final dataset (specifically rows and columns), it was possible to ensure that all data was loaded corretly </p> \n",
    "<p> <b> 2. Identifying missing values: </b> By making use of '.isnull()' method, it was possible to identify missing values and then count them with the '.sum()' method to check missing values in each column. It is also possible to check that there was no records of missing values, but important to highlight that the values were also filled by 0, null and 'none' variables. Therefore, needing a deeper analysis over that. </p> \n",
    "<p> <b> 3. Checking data types:</b> There was a check done for all data types, it was used 'df.dtypes' to recognize all different data types to be used on the analysis and model training. </p> \n",
    "<p> <b> 4. Identifying and handling duplicated values:</b> It was done a deeper analysis over the duplicated values, the duplicated values returned an empty dataframe on the first try, which meant that there was no duplicated rows. Secondly it was requested a count of such repeted values, which then returned 0, confirming that there was no repeted values. </p> \n",
    "<p> <b> 5. Checking for data normalization and inconsistent values:</b> This step was also done while checking the data types, which included an overview of full dataframe data types for further analysis. See output of code above for further details. </p> \n",
    "<p> <b> 6. Identifying outliers:</b> It was creted over the last few lines of code an outliers detection and count, printing 0 on the output, as well as no outliers appered over the visualization, it was also created a graph to visualize such data, but no outliers were shown. </p> \n",
    "<b> Data Validation </b>\n",
    "<p> This step of the data cleaning was done a check for unique values by the end of â€ he code so far it was done just to give a general idea of the data to be used. This step outputed unique values found in Gender, Alcohool Consumption, Physical Activity, Diet, Obesity, and Diagnosed Disease. Such values were broken down and outputed together with each category.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59ecedc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Male'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bl/19n82dfx2jdcfq3h1tg_4jzh0000gn/T/ipykernel_5174/219297760.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Normalising the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1472\u001b[0m                 )\n\u001b[1;32m   1473\u001b[0m             ):\n\u001b[0;32m-> 1474\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \"\"\"\n\u001b[1;32m    911\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    913\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    995\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNpDtype\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m     def __array_wrap__(\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Male'"
     ]
    }
   ],
   "source": [
    "# double check whole code \n",
    "# Importting libraries for training and testing models \n",
    "from sklearn.model_selection import train_test_split\n",
    "# Importing libraries for encoding data into numerical vlaues\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Create preprocessing pipeline for numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create preprocessing pipeline for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps for both numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# double check this step \n",
    "\n",
    "# Preprocess the data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X = df.drop('Diagnosed Disease', axis=1)\n",
    "y = df['Diagnosed Disease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Normalising the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa6514",
   "metadata": {},
   "source": [
    "<h3> Explanation of the code provided above to perform the models trainings </h3> \n",
    "<h4> Data Normalization explanation </h4>\n",
    "<p> </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d90dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate the models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    results[name] = accuracy\n",
    "    print(f'{name} Accuracy: {accuracy}')\n",
    "    print(f'Confusion Matrix for {name}:\\n', confusion_matrix(y_test, predictions))\n",
    "    print(f'Classification Report for {name}:\\n', classification_report(y_test, predictions, zero_division=0))\n",
    "\n",
    "# Plotting the accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(results.keys(), results.values())\n",
    "plt.title('Comparison of Model Accuracies')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Making a prediction sample\n",
    "sample = X_test[0].reshape(1, -1)  # Reshape for a single sample\n",
    "for name, model in models.items():\n",
    "    prediction = model.predict(sample)\n",
    "    print(f'{name} prediction for the first sample: {prediction}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c2a25",
   "metadata": {},
   "source": [
    "<h3> Explanation of the code provided above to perform the models </h3> \n",
    "<p> </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7bdd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7a204f5",
   "metadata": {},
   "source": [
    "<h4> Making use of Dashboards </h4>\n",
    "<p> For creating the dashboards in this project, we made use of some tools like Plotly. We designed an iteractive dashboard to display user input data, visualize trands in meals, exercise, and lifestyle habits, and present predictive analytics for disease risk assessment. The following dashboards provide users with a comprehensive overview of their health status, and a proactive management for the insights. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be87cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been modified and saved as 'Modified_Health_Data_50k_English.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Health_Data_50k_English.csv')\n",
    "\n",
    "# Modify the age by adding random noise within +/- 5 years\n",
    "np.random.seed(42)\n",
    "data['Age'] += np.random.randint(-5, 6, size=data.shape[0])\n",
    "\n",
    "# Modify Smoking Years by adding/subtracting within a range\n",
    "data['Smoking Years'] += np.random.randint(-3, 4, size=data.shape[0])\n",
    "data['Smoking Years'] = data['Smoking Years'].clip(lower=0)\n",
    "\n",
    "# Randomly assign new values to 'Physical Activity' and 'Diet'\n",
    "activities = data['Physical Activity'].unique()\n",
    "diets = data['Diet'].unique()\n",
    "data['Physical Activity'] = np.random.choice(activities, size=data.shape[0])\n",
    "data['Diet'] = np.random.choice(diets, size=data.shape[0])\n",
    "\n",
    "# Simulate disease diagnosis based on lifestyle factors\n",
    "conditions = ['None', 'Diabetes', 'Cardiovascular Disease', 'Hypertension', 'Lung Cancer']\n",
    "probabilities = [0.7, 0.1, 0.1, 0.1]  # Adjust probabilities as necessary\n",
    "data['Diagnosed Disease'] = np.random.choice(conditions, size=data.shape[0], p=probabilities)\n",
    "\n",
    "# Save the modified dataset\n",
    "data.to_csv('Modified_Health_Data_50k_English.csv', index=False)\n",
    "\n",
    "print(\"Dataset has been modified and saved as 'Modified_Health_Data_50k_English.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e399574",
   "metadata": {},
   "source": [
    "<h4> Code explanation and visualization overview </h4>\n",
    "<p>  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb524fa",
   "metadata": {},
   "source": [
    "<h4> Machine Learning - Classification Model </h4>\n",
    "<p> The chosen model is <b> Random Forest Classifier</b>, due to its accuracy, and ability to handle both numerical and categorical features in an efficient way. The model was decided taking into consideration that our project involves predicting the risk of developing certain diseases based on lifestyle habits and nutritional information. Therefore the project will be based on the predictive modelling. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2d388",
   "metadata": {},
   "source": [
    "<h4> Addition of a dataset, it will be used for the disease prediction along with user gathered data </h4>\n",
    "<p> The chosen extra dataset is    which will be merged to our current 'user_data.csv', for then predicting what will be required. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6524831",
   "metadata": {},
   "source": [
    "<h4> Explanation of the code above - classification model performance </h4>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
